{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Re**current **G**ener**A**tive C**L**assifier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq7wVuA6jSrM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "\n",
        "from models.ReGAL import ReGALModel\n",
        "from utils.visualize import imshow_mnist, imshow_cifar10\n",
        "from utils.data_loaders import get_mnist_data_loaders, get_cifar10_data_loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHvbR-A1EBTV",
        "outputId": "9ef0327a-63cd-4cc8-d868-9ff1d0763ad3"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 32\n",
        "%env \"WANDB_NOTEBOOK_NAME\" \"main.ipynb\"\n",
        "wandb.login()\n",
        "\n",
        "print(f\"... Running on {DEVICE} ...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_train_mnist_loader, X_test_mnist_loader, classes_mnist = get_mnist_data_loaders(batch_size=BATCH_SIZE, root_path=\"data/\", download=False)\n",
        "X_train_cifar_loader, X_test_cifar_loader, classes_cifar = get_cifar10_data_loaders(batch_size=BATCH_SIZE, root_path=\"data/\", download=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config_dict = {\n",
        "  \"classifier_cnn_layers\": (16,64,32),\n",
        "  \"classifier_cnn_input_dims\": (32,32,3),\n",
        "  \"classifier_cnn_output_dim\": 512,\n",
        "  \"classifier_head_layers\": (128,64,32,10),\n",
        "  \"generator_cnn_block_in_layer_shapes\": (512, 458),\n",
        "  \"generator_prediction_in_layer_shapes\": (10,64),\n",
        "  \"generator_in_combined_main_layer_shapes\": (522,1024),\n",
        "  \"generator_cnn_trans_layer_shapes\": (32,32,16,3),\n",
        "  \"generator_input_dims\":(16,8,8),\n",
        "  \"classifier_lr\": 0.001,\n",
        "  \"classifier_weight_decay\": 1e-5,\n",
        "  \"generator_lr\": 0.004,\n",
        "  \"generator_weight_decay\": 1e-5,\n",
        "  \"eval_run_classifier_cnn_block_optimizer_lr\": 0.008,\n",
        "  \"eval_run_classifier_cnn_block_optimizer_weight_decay\": 1e-5,\n",
        "  \"eval_run_classifier_head_block_optimizer_lr\": 0.009,\n",
        "  \"eval_run_classifier_head_block_optimizer_weight_decay\": 1e-5,\n",
        "  \"device\": DEVICE,\n",
        "  \"verbose\": False\n",
        "}\n",
        "\n",
        "model = ReGALModel(config_dict=config_dict)\n",
        "model.load_pretrained_params(\"model_parameters/model_parameters_dict_checkpoint_cifar10_20-12-2021_0.20.tar\", load_optimizers=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Pretrain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run = wandb.init(project=\"ReGAL\", entity=\"johnny1188\", tags=[\"pretraining\"], config=config_dict)\n",
        "# wandb.watch(models=(\n",
        "#   model.classifier['cnn_block'], model.classifier['head_block'],\n",
        "#   model.generator['head_block'],\n",
        "#   model.generator['head_block'].dense_layers_stack_dict[\"in_classifier_prediction\"],\n",
        "#   model.generator['head_block'].dense_layers_stack_dict[\"in_combined_main_stack\"],\n",
        "#   model.generator['trans_cnn_block']), log=\"all\", log_freq=500)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_history, samples = model.pretrain(\n",
        "    epochs=10,\n",
        "    X_train_loader=X_train_cifar_loader,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    past_loss_history=None,\n",
        "    verbose=True,\n",
        "    wandb_run=False\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73hipcpyPec2"
      },
      "source": [
        "### Save pretrained model's parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "now = datetime.now()\n",
        "model.save_model_params(f\"model_parameters/model_parameters_dict_checkpoint_cifar10_{now.day}-{now.month}-{now.year}_{now.hour}.{now.minute}.tar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8gV2WxksUj4"
      },
      "source": [
        "# Analysis of pretraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "1s4QwZzysZVs",
        "outputId": "537b7578-71be-4baf-a611-6b89f7500984"
      },
      "outputs": [],
      "source": [
        "fig,ax = plt.subplots(nrows=1,ncols=2,figsize=(25,8))\n",
        "ax[0].plot([t.cpu().detach() for t in loss_history[\"classifier\"]])\n",
        "ax[0].title.set_text(\"classifier loss\")\n",
        "ax[1].plot([t.cpu().detach() for t in loss_history[\"generator\"]])\n",
        "ax[1].title.set_text(\"generator loss\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 545
        },
        "id": "ktbuRaEfQ_pp",
        "outputId": "f4b527f1-cb12-494f-b9c1-3e3b70174a7f"
      },
      "outputs": [],
      "source": [
        "epoch = 9\n",
        "i = 0\n",
        "imshow_cifar10(\n",
        "    samples[epoch][0][i:i+5].cpu().detach(), \n",
        "    [f\"Ground truth: {classes_cifar[ samples[epoch][2][i].cpu().detach() ]}\",\n",
        "    f\"Ground truth: {classes_cifar[ samples[epoch][2][i+1].cpu().detach() ]}\",\n",
        "    f\"Ground truth: {classes_cifar[ samples[epoch][2][i+2].cpu().detach() ]}\",\n",
        "    f\"Ground truth: {classes_cifar[ samples[epoch][2][i+3].cpu().detach() ]}\",\n",
        "    f\"Ground truth: {classes_cifar[ samples[epoch][2][i+4].cpu().detach() ]}\",],\n",
        "    w_color=True)\n",
        "imshow_cifar10(\n",
        "    samples[epoch][1][i:i+5].cpu().detach(), \n",
        "    [f\"Reconstruction: {classes_cifar[ torch.argmax(samples[epoch][3][i].cpu().detach()) ]}\",\n",
        "    f\"Reconstruction: {classes_cifar[ torch.argmax(samples[epoch][3][i+1].cpu().detach()) ]}\",\n",
        "    f\"Reconstruction: {classes_cifar[ torch.argmax(samples[epoch][3][i+2].cpu().detach()) ]}\",\n",
        "    f\"Reconstruction: {classes_cifar[ torch.argmax(samples[epoch][3][i+3].cpu().detach()) ]}\",\n",
        "    f\"Reconstruction: {classes_cifar[ torch.argmax(samples[epoch][3][i+4].cpu().detach()) ]}\"],\n",
        "    w_color=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bo3UzMp69rZa"
      },
      "outputs": [],
      "source": [
        "images = iter(X_train_cifar_loader)\n",
        "X, y = next(images)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print( model.generator[\"head_block\"].dense_layers_stack_dict[\"in_combined_main_stack\"][0].weight.shape )\n",
        "plt.plot( model.generator[\"head_block\"].dense_layers_stack_dict[\"in_combined_main_stack\"][0].weight[:,-30:-20].detach().cpu() )\n",
        "# torch.mean(torch.abs( model.generator[\"head_block\"].dense_layers_stack_dict[\"in_combined_main_stack\"][0].weight[:,:10] ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images = iter(X_train_cifar_loader)\n",
        "X, y = [part_of_data.to(model.device) for part_of_data in next(images)]\n",
        "\n",
        "z = model.classifier[\"cnn_block\"](X)\n",
        "z = z.reshape((BATCH_SIZE, model.classifier_cnn_output_dim_flattened))\n",
        "y_hat = model.classifier[\"head_block\"](z)\n",
        "\n",
        "# Normal reconstruction as in the pretraining and evaluation phases\n",
        "h = model.generator[\"head_block\"](z.detach(), y_hat.detach()) # TODO: Try to pretrain the generator w/ true target labels\n",
        "h_reshaped_for_cnn_block = torch.reshape(h, (BATCH_SIZE, *model.generator_cnn_input_dims))\n",
        "X_hat = model.generator[\"trans_cnn_block\"](h_reshaped_for_cnn_block)\n",
        "\n",
        "one_hot = torch.zeros(y_hat.shape)\n",
        "preds = torch.argmax(y_hat,dim=1)\n",
        "for i in preds:\n",
        "  one_hot[i,(preds[i] + 3) % len(one_hot[i])] = 1\n",
        "\n",
        "# Permutation of the last ten values of the generator's head block (=permuted categories)\n",
        "# h_2 = model.generator[\"head_block\"](z.detach(), one_hot.to(model.device)) # TODO: Try to pretrain the generator w/ true target labels\n",
        "h_2 = model.generator[\"head_block\"](z.detach(), torch.zeros(y_hat.shape).to(model.device)) # TODO: Try to pretrain the generator w/ true target labels\n",
        "h_reshaped_for_cnn_block_2 = torch.reshape(h_2, (BATCH_SIZE, *model.generator_cnn_input_dims))\n",
        "X_hat_2 = model.generator[\"trans_cnn_block\"](h_reshaped_for_cnn_block_2)\n",
        "\n",
        "imshow_cifar10(\n",
        "    X[0:5].cpu().detach(), \n",
        "    f\"Ground truth\",\n",
        "    w_color=True\n",
        ")\n",
        "imshow_cifar10(\n",
        "    X_hat[0:5].cpu().detach(), \n",
        "    f\"Gen\",\n",
        "    w_color=True\n",
        ")\n",
        "imshow_cifar10(\n",
        "    X_hat_2[0:5].cpu().detach(), \n",
        "    f\"Gen (changed category)\",\n",
        "    w_color=True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5lRr7Febp-b"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPQHAWNh5Mgw"
      },
      "outputs": [],
      "source": [
        "def eval(model, X_test_loader, max_reconstruction_steps=10, max_batches=200):\n",
        "  model.turn_components_to_eval_mode()\n",
        "\n",
        "  loss_func_classification = nn.CrossEntropyLoss()\n",
        "  classification_loss_history = []\n",
        "\n",
        "  for i,data in enumerate(X_test_loader):\n",
        "    X, y = [part_of_data.to(DEVICE) for part_of_data in data]\n",
        "\n",
        "    y_hat = model(X, max_reconstruction_steps=max_reconstruction_steps)\n",
        "\n",
        "    classification_loss_history.append( loss_func_classification(y_hat, y).detach().cpu().item() )\n",
        "    if i > max_batches: break\n",
        "\n",
        "  return(classification_loss_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "id": "6FyraZL35r5l",
        "outputId": "e7d1c7a6-eb98-4127-adf7-4e3b0e46ec8e"
      },
      "outputs": [],
      "source": [
        "classification_loss_history_wout_reconstruction = eval(model, X_test_loader=X_test_cifar_loader, max_reconstruction_steps=0, max_batches=150)\n",
        "classification_loss_history_w_reconstruction = eval(model, X_test_loader=X_test_cifar_loader, max_reconstruction_steps=15, max_batches=150)\n",
        "\n",
        "print(\n",
        "f\"\"\"-----\\nMean classification loss:\n",
        ">>> with reconstruction: {round(sum(classification_loss_history_w_reconstruction)/len(classification_loss_history_w_reconstruction), 4)}\n",
        ">>> without reconstruction: {round(sum(classification_loss_history_wout_reconstruction)/len(classification_loss_history_wout_reconstruction), 4)}\\n-----\\n\"\"\"\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(classification_loss_history_w_reconstruction, label=\"with reconstruction\")\n",
        "plt.plot(classification_loss_history_wout_reconstruction, label=\"without reconstruction\")\n",
        "plt.legend()\n",
        "plt.title(\"Classification loss history\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cXuZbZ-ijYyF",
        "q7fPzuEtSwGR",
        "KRGMHyTeQ7RY",
        "ZskRT7uhjr7r",
        "LIBwcuXvjxJ1",
        "9k0BjbB_opsC",
        "OmykYP4-xZac",
        "2CGy2JNabcb8",
        "jfEvX78sNQCg"
      ],
      "name": "main.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
