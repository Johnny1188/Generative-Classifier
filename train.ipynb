{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## **Generative Classifier**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aq7wVuA6jSrM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchsummary import summary\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "import wandb\n",
        "\n",
        "from models.GenClassifier import GenClassifier\n",
        "\n",
        "from train import train\n",
        "\n",
        "from utils.visualize import show_img, plot_loss_history, show_samples, plot_weights, plot_gradual_classification_loss, plot_conv_channels\n",
        "from utils.data_loaders import get_mnist_data_loaders, get_cifar10_data_loaders, get_fashion_mnist_data_loaders\n",
        "from utils.other import calc_accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHvbR-A1EBTV",
        "outputId": "9ef0327a-63cd-4cc8-d868-9ff1d0763ad3"
      },
      "outputs": [],
      "source": [
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "BATCH_SIZE = 32\n",
        "# BATCH_SIZE = 16\n",
        "%env \"WANDB_NOTEBOOK_NAME\" \"main.ipynb\"\n",
        "wandb.login()\n",
        "\n",
        "print(f\"... Running on {DEVICE} ...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# X_train_loader, X_test_loader, class_names = get_mnist_data_loaders(batch_size=BATCH_SIZE, root_path=\"data/\", download=False)\n",
        "# X_train_loader, X_test_loader, class_names = get_cifar10_data_loaders(batch_size=BATCH_SIZE, root_path=\"data/\", download=False)\n",
        "X_train_loader, X_test_loader, class_names = get_fashion_mnist_data_loaders(batch_size=BATCH_SIZE, root_path=\"data/\", download=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "config_dict = {\n",
        "  \"classifier_cnn_layers\": (32,64,4),\n",
        "  \"classifier_cnn_input_dims\": (28,28,1),\n",
        "  \"classifier_cnn_output_dim\": 144,\n",
        "  \"classifier_head_layers\": (64,128,10),\n",
        "  \"generator_cnn_block_in_layer_shapes\": (144,240),\n",
        "  \"generator_prediction_in_layer_shapes\": (10,72),\n",
        "  \"generator_in_combined_main_layer_shapes\": (312,512,288),\n",
        "  \"generator_trans_cnn_input_dims\":(2,12,12),\n",
        "  \"generator_cnn_trans_layer_shapes\": (32,64,32,1),\n",
        "  \"classifier_lr\": 0.0008,\n",
        "  \"classifier_weight_decay\": 1e-5,\n",
        "  \"generator_reconstruction_loss_importance\": 1.2,\n",
        "  \"generator_reconstruction_from_no_z_loss_importance\": 1.2,\n",
        "  \"generator_classification_loss_importance\": 0.6,\n",
        "  \"generator_z_similarity_loss_importance\": 1.0,\n",
        "  \"generator_contrastive_loss_importance\": 0.8,\n",
        "  \"generator_lr\": 0.001,\n",
        "  \"generator_weight_decay\": 1e-5,\n",
        "  \"eval_run_classifier_cnn_block_optimizer_lr\": 0.008,\n",
        "  \"eval_run_classifier_cnn_block_optimizer_weight_decay\": 1e-5,\n",
        "  \"eval_run_classifier_head_block_optimizer_lr\": 0.015,\n",
        "  \"eval_run_classifier_head_block_optimizer_weight_decay\": 1e-5,\n",
        "  \"device\": DEVICE\n",
        "}\n",
        "\n",
        "model = GenClassifier(config_dict=config_dict)\n",
        "model.load_pretrained_params(\"model_parameters/fashion_mnist_23-1-2022_11.27.tar\", load_optimizers=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb_run = wandb.init(\n",
        "    project=\"ReGAL\", entity=\"johnny1188\", config=config_dict,\n",
        "    group=\"fashion-mnist\",\n",
        "    tags=[\"pretraining\", \"gen-four-step-loss\"],\n",
        "    notes=f\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_history, samples = train(\n",
        "    model,\n",
        "    epochs=15,\n",
        "    X_train_loader=X_train_loader,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    verbose=True,\n",
        "    is_wandb_run=True,\n",
        "    class_names=class_names\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "wandb_run.finish()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73hipcpyPec2"
      },
      "source": [
        "### Save pretrained model's parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "now = datetime.now()\n",
        "\n",
        "# model.save_model_params(f\"model_parameters/cifar10_{now.day}-{now.month}-{now.year}_{now.hour}.{now.minute}.tar\")\n",
        "# model.save_model_params(f\"model_parameters/mnist_{now.day}-{now.month}-{now.year}_{now.hour}.{now.minute}.tar\")\n",
        "model.save_model_params(f\"model_parameters/fashion_mnist_{now.day}-{now.month}-{now.year}_{now.hour}.{now.minute}.tar\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8gV2WxksUj4"
      },
      "source": [
        "### Analysis of training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "show_samples(samples, 9, 0, 5, [*class_names,\"_from_generator\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check weights\n",
        "plot_weights(model.generator[\"head_block\"].dense_layers_stack_dict[\"in_combined_main_stack\"][0].weight.detach().cpu().numpy())\n",
        "\n",
        "print(torch.mean(torch.abs(model.generator[\"head_block\"].dense_layers_stack_dict[\"in_combined_main_stack\"][0].weight[:,:73])))\n",
        "print(torch.mean(torch.abs(model.generator[\"head_block\"].dense_layers_stack_dict[\"in_combined_main_stack\"][0].weight[:,73:])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.turn_model_to_mode(\"eval\")\n",
        "\n",
        "images = iter(X_test_loader)\n",
        "X, y = [part_of_data.to(model.config_dict[\"device\"]) for part_of_data in next(images)]\n",
        "\n",
        "z = model.classifier[\"cnn_block\"](X)\n",
        "z = z.reshape((BATCH_SIZE, model.config_dict[\"classifier_cnn_output_dim\"]))\n",
        "y_hat = model.classifier[\"head_block\"](z)\n",
        "\n",
        "# Normal reconstruction as in the pretraining and evaluation phases\n",
        "y_onehot = nn.functional.one_hot(y,10).float().to(model.config_dict[\"device\"])\n",
        "# h = model.generator[\"head_block\"](z.detach(), y_onehot.detach())\n",
        "h = model.generator[\"head_block\"](z.detach(), nn.functional.softmax(y_hat.detach(),dim=1))\n",
        "h_reshaped_for_cnn_block = torch.reshape(h, (BATCH_SIZE, *model.config_dict[\"generator_trans_cnn_input_dims\"]))\n",
        "X_hat = model.generator[\"trans_cnn_block\"](h_reshaped_for_cnn_block)\n",
        "\n",
        "# Permutation of the last ten values of the generator's head block (=permuted categories)\n",
        "y_one_hot_cloned = np.random.permutation( y_onehot.clone().detach().cpu().numpy() )\n",
        "y_onehot_permuted_classes = torch.tensor(y_one_hot_cloned).to(model.config_dict[\"device\"])\n",
        "# h_2 = model.generator[\"head_block\"](z.detach(), torch.randint_like(y, 9).to(model.config_dict[\"device\"]))\n",
        "h_2 = model.generator[\"head_block\"](z.detach(), y_onehot_permuted_classes)\n",
        "h_reshaped_for_cnn_block_2 = torch.reshape(h_2, (BATCH_SIZE, *model.config_dict[\"generator_trans_cnn_input_dims\"]))\n",
        "X_hat_2 = model.generator[\"trans_cnn_block\"](h_reshaped_for_cnn_block_2)\n",
        "\n",
        "# Zeroing-out cnn input and generating from the true target labels (categories)\n",
        "h_3 = model.generator[\"head_block\"](torch.zeros(z.detach().shape).to(model.config_dict[\"device\"]), y_onehot.detach())\n",
        "h_reshaped_for_cnn_block_3 = torch.reshape(h_3, (BATCH_SIZE, *model.config_dict[\"generator_trans_cnn_input_dims\"]))\n",
        "X_hat_3 = model.generator[\"trans_cnn_block\"](h_reshaped_for_cnn_block_3)\n",
        "\n",
        "# Zeroing-out cnn input and generating from the predicted categories\n",
        "h_4 = model.generator[\"head_block\"](torch.zeros(z.detach().shape).to(model.config_dict[\"device\"]), nn.functional.softmax(y_hat.detach(),dim=1))\n",
        "h_reshaped_for_cnn_block_4 = torch.reshape(h_4, (BATCH_SIZE, *model.config_dict[\"generator_trans_cnn_input_dims\"]))\n",
        "X_hat_4 = model.generator[\"trans_cnn_block\"](h_reshaped_for_cnn_block_4)\n",
        "\n",
        "# indexes of imgs to show\n",
        "img_i_start = 0\n",
        "img_i_end = 5\n",
        "\n",
        "print(\"Input images:\")\n",
        "show_img(\n",
        "    X[img_i_start:img_i_end].cpu().detach(), \n",
        "    [class_names[c_i] for c_i in y_onehot[img_i_start:img_i_end].detach().cpu().argmax(axis=1)]\n",
        ")\n",
        "print(\"Generated - w/ predicted categories:\")\n",
        "show_img(\n",
        "    X_hat[img_i_start:img_i_end].cpu().detach(),\n",
        "    [class_names[c_i] for c_i in y_hat[img_i_start:img_i_end].detach().cpu().argmax(axis=1)]\n",
        ")\n",
        "print(\"Generated - w/ permuted true target categories:\")\n",
        "show_img(\n",
        "    X_hat_2[img_i_start:img_i_end].cpu().detach(),\n",
        "    [class_names[c_i] for c_i in y_one_hot_cloned[img_i_start:img_i_end].argmax(axis=1)]\n",
        ")\n",
        "print(\"Generated - w/ zeroed-out cnn input & true target categories:\")\n",
        "show_img(\n",
        "    X_hat_3[img_i_start:img_i_end].cpu().detach(), \n",
        "    [class_names[c_i] for c_i in y_onehot[img_i_start:img_i_end].detach().cpu().argmax(axis=1)]\n",
        ")\n",
        "print(\"Generated - w/ zeroed-out cnn input & predicted categories:\")\n",
        "show_img(\n",
        "    X_hat_4[img_i_start:img_i_end].cpu().detach(), \n",
        "    [class_names[c_i] for c_i in y_hat[img_i_start:img_i_end].detach().cpu().argmax(axis=1)]\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cXuZbZ-ijYyF",
        "q7fPzuEtSwGR",
        "KRGMHyTeQ7RY",
        "ZskRT7uhjr7r",
        "LIBwcuXvjxJ1",
        "9k0BjbB_opsC",
        "OmykYP4-xZac",
        "2CGy2JNabcb8",
        "jfEvX78sNQCg"
      ],
      "name": "main.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
